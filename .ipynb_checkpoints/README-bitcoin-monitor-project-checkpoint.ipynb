{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c9ce8b",
   "metadata": {},
   "source": [
    "# Bitcoin Monitor Pipeline Project\n",
    "\n",
    "Credit to @josephmachado for project concept\n",
    "\n",
    "For our project, we will pull bitcoin exchange data from [CoinCap API](https://docs.coincap.io/). We will pull this data every 5 minutes and load it into our warehouse.\n",
    "\n",
    "We use python to pull, transform and load data. Our warehouse is postgres. We also spin up a Metabase instance for our presentation layer.\n",
    "\n",
    "All of the components are running as docker containers.\n",
    "\n",
    "![Pipeline](https://www.startdataengineering.com/images/bc_monitor/bc_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e84afb",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. [Docker](https://docs.docker.com/engine/install/) and [Docker Compose](https://docs.docker.com/compose/install/) v1.27.0 or later.\n",
    "2. [AWS account](https://aws.amazon.com/)\n",
    "3. [AWS CLI installed](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) and [configured](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)\n",
    "4. [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7ca60",
   "metadata": {},
   "source": [
    "## ETL Code\n",
    "\n",
    "The code to pull data from [CoinCap API](https://docs.coincap.io/) and load it into our warehouse is at `exchange_data_etl.py`. In this script we\n",
    "\n",
    "1. Pull data from [CoinCap API](https://docs.coincap.io/) using the `get_exchange_data` function.\n",
    "2. Use `get_utc_from_unix_time` function to get UTC based date time from unix time(in ms).\n",
    "3. Load data into our warehouse using the `_get_exchange_insert_query` insert query.\n",
    "\n",
    "```python\n",
    "def run() -> None:\n",
    "    data = get_exchange_data()\n",
    "    for d in data:\n",
    "        d['update_dt'] = get_utc_from_unix_time(d.get('updated'))\n",
    "    with WarehouseConnection(**get_warehouse_creds()).managed_cursor() as curr:\n",
    "        p.execute_batch(curr, _get_exchange_insert_query(), data)\n",
    "```\n",
    "\n",
    "There are a few things going on at `“with WarehouseConnection(**get_warehouse_creds()).managed_cursor() as curr:\"`.\n",
    "\n",
    "1. We use the `get_warehouse_creds` utility function to get the warehouse connection credentials.\n",
    "2. The warehouse connection credentials are stored as environment variables within our docker compose definition. The `docker-compose` uses the hardcoded values from the `env` file.\n",
    "3. The credentials are passed as `**kwargs` to the `WarehouseConnection` class.\n",
    "4. The `WarehouseConnection` class uses contextmanager to enable opening and closing the DB connections easier. This lets us access the DB connection without having to write boilerplate code.\n",
    "\n",
    "```python\n",
    "def get_warehouse_creds() -> Dict[str, Optional[Union[str, int]]]:\n",
    "    return {\n",
    "        'user': os.getenv('WAREHOUSE_USER'),\n",
    "        'password': os.getenv('WAREHOUSE_PASSWORD'),\n",
    "        'db': os.getenv('WAREHOUSE_DB'),\n",
    "        'host': os.getenv('WAREHOUSE_HOST'),\n",
    "        'port': int(os.getenv('WAREHOUSE_PORT', 5432)),\n",
    "    }\n",
    "```\n",
    "\n",
    "```python\n",
    "class WarehouseConnection:\n",
    "    def __init__(\n",
    "        self, db: str, user: str, password: str, host: str, port: int\n",
    "    ):\n",
    "        self.conn_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n",
    "\n",
    "    @contextmanager\n",
    "    def managed_cursor(self, cursor_factory=None):\n",
    "        self.conn = psycopg2.connect(self.conn_url)\n",
    "        self.conn.autocommit = True\n",
    "        self.curr = self.conn.cursor(cursor_factory=cursor_factory)\n",
    "        try:\n",
    "            yield self.curr\n",
    "        finally:\n",
    "            self.curr.close()\n",
    "            self.conn.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f06637",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "We will add 2 major types of tests:\n",
    "\n",
    "1. Unit test: To test if individual functions are working as expected. We test `get_utc_from_unix_time` with the `test_get_utc_from_unix_time` function.\n",
    "2. Integration test: To test if multiple systems work together as expected.\n",
    "\n",
    "For the integration test we:\n",
    "\n",
    "1. Mock the Coinbase API call using the `mocker` functionality of the `pytest-mock` library. We use fixture data at `test/fixtures/sample_raw_exchange_data.csv` as a result of an API call. This is to enable deterministic testing.\n",
    "2. Assert that the data we store in the warehouse is the same as we expected.\n",
    "3. Finally the `teardown_method` truncates the local warehouse table. This is automatically called by pytest after the `test_covid_stats_etl_run` test function is run.\n",
    "\n",
    "```python\n",
    "class TestBitcoinMonitor:\n",
    "    def teardown_method(self, test_covid_stats_etl_run):\n",
    "        with WarehouseConnection(\n",
    "            **get_warehouse_creds()\n",
    "        ).managed_cursor() as curr:\n",
    "            curr.execute(\"TRUNCATE TABLE bitcoin.exchange;\")\n",
    "\n",
    "    def get_exchange_data(self):\n",
    "        with WarehouseConnection(**get_warehouse_creds()).managed_cursor(\n",
    "            cursor_factory=psycopg2.extras.DictCursor\n",
    "        ) as curr:\n",
    "            curr.execute(\n",
    "                '''SELECT id,\n",
    "                        name,\n",
    "                        rank,\n",
    "                        percenttotalvolume,\n",
    "                        volumeusd,\n",
    "                        tradingpairs,\n",
    "                        socket,\n",
    "                        exchangeurl,\n",
    "                        updated_unix_millis,\n",
    "                        updated_utc\n",
    "                        FROM bitcoin.exchange;'''\n",
    "            )\n",
    "            table_data = [dict(r) for r in curr.fetchall()]\n",
    "        return table_data\n",
    "\n",
    "    def test_covid_stats_etl_run(self, mocker):\n",
    "        mocker.patch(\n",
    "            'bitcoinmonitor.exchange_data_etl.get_exchange_data',\n",
    "            return_value=[\n",
    "                r\n",
    "                for r in csv.DictReader(\n",
    "                    open('test/fixtures/sample_raw_exchange_data.csv')\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        run()\n",
    "        expected_result = [\n",
    "          {\"see github repo for full data\"}\n",
    "        ]\n",
    "        result = self.get_exchange_data()\n",
    "        assert expected_result == result\n",
    "```\n",
    "\n",
    "Run tests using:\n",
    "```bash\n",
    "make up # to start all your containers \n",
    "make pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbe895",
   "metadata": {},
   "source": [
    "## Scheduler\n",
    "\n",
    "Now that we have the ETL script and tests setup, we need to schedule the ETL script to run every 5 minutes. Since this is a simple script we will go with `cron` instead of setting up a framework like Airflow or Dagster. The cron job is defined at `scheduler/pull_bitcoin_exchange_info`:\n",
    "\n",
    "```bash\n",
    "SHELL=/bin/bash\n",
    "HOME=/\n",
    "*/5 * * * * WAREHOUSE_USER=sdeuser WAREHOUSE_PASSWORD=sdepassword1234 WAREHOUSE_DB=finance WAREHOUSE_HOST=warehouse WAREHOUSE_PORT=5432  PYTHONPATH=/code/src /usr/local/bin/python /code/src/bitcoinmonitor/exchange_data_etl.py\n",
    "```\n",
    "\n",
    "This file is placed inside the `pipelinerunner` docker container’s `crontab` location. You may notice that we have hardcoded the environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9fca4",
   "metadata": {},
   "source": [
    "## Presentation\n",
    "\n",
    "Now that we have the code and scheduler set up, we can add checks and formatting automation to ensure that we follow best practices.\n",
    "\n",
    "### Formatting, Linting, and Type checks\n",
    "Formatting enables us to stay consistent with the code format. We use black and isort to automate formatting. The `-S` black module flag ensures that we use single quotes for strings (following PEP8).\n",
    "\n",
    "Linting analyzes the code for potential errors and ensures that the code formatting is consistent. We use `flake8` to lint check our code.\n",
    "\n",
    "Type checking enables us to catch type errors (when defined). We use `mypy` for this.\n",
    "\n",
    "All of these are run within the docker container. We use a `Makefile` to store shortcuts to run these commands.\n",
    "\n",
    "Automatically format and test the code:\n",
    "```bash\n",
    "make ci # this command will format your code, run lint and type checks and run all your tests \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9c610",
   "metadata": {},
   "source": [
    "## Live Dashboard\n",
    "\n",
    "We have a [`Makefile`](Makefile) with common commands. These are executed in the running container.\n",
    "\n",
    "We can spin up a dashboard locally using:\n",
    "```bash\n",
    "make up # starts all the containers\n",
    "make ci # runs formatting, lint check, type check and python test\n",
    "```\n",
    "\n",
    "Then visit [http://localhost:3000](http://localhost:3000) to log into your local Metabase instance. From the Metabase UI, you can add charts and dashboards. \n",
    "\n",
    "You can connect to the warehouse with the following credentials:\n",
    "```bash\n",
    "Host: warehouse\n",
    "Database name: finance\n",
    "```\n",
    "\n",
    "The remaining configs are available in the [env](env) file.\n",
    "\n",
    "To tear down the local containers you can use:\n",
    "```bash\n",
    "make down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e9c6b",
   "metadata": {},
   "source": [
    "## Deploy to Production\n",
    "\n",
    "Now that you have your code working locally, it’s time to deploy to production. We will run our data pipeline and dashboards as containers on an EC2 instance. We have helper scripts in [deploy_helpers](deploy_helpers) for this.  The first step is to start an EC2 ubuntu instance.\n",
    "\n",
    "In EC2, we create an instance. A t2.micro instance should suffice (if you plan to keep your ETL job running indefinitely, you might need a bigger machine).\n",
    "\n",
    "In the security group section, add a TCP rule, on port 3000 that accepts inbound connections from any `0.0.0.0/0` address.\n",
    "\n",
    "Wait for the EC2 instance to be in a ready state with all the checks passed. Then note down its `public DNS` and `public IPV4` addresses.\n",
    "\n",
    "Now that you have a running EC2 instance, you can set up a prod instance as shown below.\n",
    "\n",
    "1. Deploy your code to EC2, as shown:\n",
    "```bash\n",
    "chmod 755 ./deploy_helpers/send_code_to_prod.sh\n",
    "chmod 400 your-pem-file-full-location\n",
    "./deploy_helpers/send_code_to_prod.sh your-pem-file-full-location your-EC2-Public-DNS\n",
    "# this will open up a bash to your remote EC2 instance\n",
    "# If you are having trouble connecting use method 2 from https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-fix-permission-denied-errors/\n",
    "# you will be logged into your EC2 instance\n",
    "```\n",
    "\n",
    "2. Install docker and start your ETL and dashboard containers on EC2.\n",
    "```bash\n",
    "chmod 755 install_docker.sh\n",
    "./install_docker.sh\n",
    "# verify that docker and docker compose installed\n",
    "docker --version\n",
    "docker-compose --version\n",
    "# start the containers\n",
    "unzip bitcoinmonitor.gzip && cd bitcoin-monitor-project/\n",
    "docker-compose --env-file env up --build -d\n",
    "```\n",
    "\n",
    "You can log into your remote Metabase instance by using `http://your-public-ipv4-address:3000`. From Metabase, you can set up a connection to the Postgres warehouse with the following credentials:\n",
    "```bash\n",
    "Host: warehouse\n",
    "Database name: finance\n",
    "```\n",
    "\n",
    "The remaining configs are available in the env file.\n",
    "\n",
    "You can spin down your local instance with:\n",
    "```bash\n",
    "make down\n",
    "```\n",
    "\n",
    "Do not forget to turn off your EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5d6da",
   "metadata": {},
   "source": [
    "### Adding Dashboard to your Profile\n",
    "\n",
    "Once you create a dashboard, get its public link following the steps [here](https://www.metabase.com/docs/latest/administration-guide/12-public-links.html). \n",
    "\n",
    "A sample dashboard using bitcoin exchange data is shown below:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Depending on the EC2 instance type you choose you may incur some cost. Use [AWS cost calculator](https://calculator.aws/#/) to figure out the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f24123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
